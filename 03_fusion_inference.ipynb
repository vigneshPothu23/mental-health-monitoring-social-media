{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b160f1d-dcdf-4f0b-993b-2989e1927b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ad8974-583a-4438-84f1-1b0395825f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Emotion model\n",
    "emotion_tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert_emotion_model\")\n",
    "emotion_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert_emotion_model\")\n",
    "emotion_model.eval()\n",
    "\n",
    "# Load Sarcasm model\n",
    "sarcasm_tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert_sarcasm_model\")\n",
    "sarcasm_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert_sarcasm_model\")\n",
    "sarcasm_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1d5fb8-49a1-444d-8339-c380ecd97282",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = [\"anger\", \"fear\", \"joy\", \"love\", \"sadness\", \"surprise\"]\n",
    "sarcasm_labels = [\"not_sarcastic\", \"sarcastic\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7612739a-1762-45d9-ae67-847c47ad9691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion_with_sarcasm(text):\n",
    "    # ----- Emotion -----\n",
    "    e_inputs = emotion_tokenizer(\n",
    "        text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        e_outputs = emotion_model(**e_inputs)\n",
    "        e_probs = F.softmax(e_outputs.logits, dim=1)[0]\n",
    "    e_idx = int(torch.argmax(e_probs))\n",
    "    emotion = emotion_labels[e_idx]\n",
    "    emotion_conf = float(e_probs[e_idx]) * 100\n",
    "\n",
    "    # ----- Sarcasm -----\n",
    "    s_inputs = sarcasm_tokenizer(\n",
    "        text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        s_outputs = sarcasm_model(**s_inputs)\n",
    "        s_probs = F.softmax(s_outputs.logits, dim=1)[0]\n",
    "    s_idx = int(torch.argmax(s_probs))\n",
    "    sarcasm = sarcasm_labels[s_idx]\n",
    "    sarcasm_conf = float(s_probs[s_idx]) * 100\n",
    "\n",
    "    return {\n",
    "        \"emotion\": emotion,\n",
    "        \"emotion_confidence\": round(emotion_conf, 2),\n",
    "        \"sarcasm\": sarcasm,\n",
    "        \"sarcasm_confidence\": round(sarcasm_conf, 2)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf9b8b58-e615-4380-b467-b7087a5a2e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emotion': 'joy',\n",
       " 'emotion_confidence': 99.29,\n",
       " 'sarcasm': 'sarcastic',\n",
       " 'sarcasm_confidence': 69.75}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_emotion_with_sarcasm(\n",
    "    \"Oh wow, just GREAT üòí another meeting that could‚Äôve been an email\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f5de65a-828e-4d78-a629-17510aaedd00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = predict_emotion_with_sarcasm(\u001b[43mtext\u001b[49m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[33m\"\u001b[39m\u001b[33msarcasm\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33msarcastic\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è Emotion may be masked due to sarcasm\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "result = predict_emotion_with_sarcasm(text)\n",
    "\n",
    "if result[\"sarcasm\"] == \"sarcastic\":\n",
    "    print(\"‚ö†Ô∏è Emotion may be masked due to sarcasm\")\n",
    "else:\n",
    "    print(\"Emotion is reliable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c87027ba-8c87-4c61-a495-fac170d94fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Oh wow, just GREAT üòí another meeting that could‚Äôve been an email\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d090bab-61b5-40a8-b6a5-90b5ee0bcadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emotion': 'joy',\n",
       " 'emotion_confidence': 99.29,\n",
       " 'sarcasm': 'sarcastic',\n",
       " 'sarcasm_confidence': 69.75}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = predict_emotion_with_sarcasm(text)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8450c135-db62-4f82-9f6b-af8c4e9c6478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emotion': 'sadness',\n",
       " 'emotion_confidence': 99.55,\n",
       " 'sarcasm': 'not_sarcastic',\n",
       " 'sarcasm_confidence': 63.2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am feeling very low and tired today üòû\"\n",
    "predict_emotion_with_sarcasm(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a9f7428-3524-4b79-8ed6-1cf7fe7ef78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "MODEL_PATH = \"distilbert_emotion_model\"\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9d5cb23-56e0-4e28-ada0-c0e2fed5180d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3887097613.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mNameError                                 Traceback (most recent call last)\u001b[39m\n                                              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "NameError                                 Traceback (most recent call last)\n",
    "Cell In[12], line 10\n",
    "      7 preds = []\n",
    "      8 true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eb07294-6e61-49ba-a343-0e64ee623430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.926875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.96      0.90      0.93       432\n",
      "        fear       0.91      0.89      0.90       387\n",
      "         joy       0.94      0.94      0.94      1072\n",
      "        love       0.82      0.84      0.83       261\n",
      "     sadness       0.94      0.98      0.96       933\n",
      "    surprise       0.85      0.81      0.83       115\n",
      "\n",
      "    accuracy                           0.93      3200\n",
      "   macro avg       0.90      0.89      0.90      3200\n",
      "weighted avg       0.93      0.93      0.93      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Imports\n",
    "# -----------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# -----------------------------\n",
    "# Load cleaned labeled dataset\n",
    "# (same dataset used in Phase-1)\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"train.txt\", sep=\";\", names=[\"text\", \"label\"])\n",
    "df = df.dropna()\n",
    "\n",
    "# -----------------------------\n",
    "# Encode labels\n",
    "# -----------------------------\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df[\"label\"].values)\n",
    "\n",
    "X = df[\"text\"].astype(str).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer\n",
    "# -----------------------------\n",
    "MODEL_PATH = \"distilbert_emotion_model\"\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    list(X_train),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    list(X_test),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset class\n",
    "# -----------------------------\n",
    "class EmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "eval_dataset = EmotionDataset(test_encodings, y_test)\n",
    "\n",
    "# -----------------------------\n",
    "# Load model\n",
    "# -----------------------------\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=len(label_encoder.classes_)\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# Predictions\n",
    "# -----------------------------\n",
    "preds = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in eval_dataset:\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[\"input_ids\"].unsqueeze(0),\n",
    "            \"attention_mask\": batch[\"attention_mask\"].unsqueeze(0)\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "    preds.append(pred)\n",
    "    true_labels.append(batch[\"labels\"].item())\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "print(\"Accuracy:\", accuracy_score(true_labels, preds))\n",
    "print(\n",
    "    classification_report(\n",
    "        true_labels,\n",
    "        preds,\n",
    "        target_names=label_encoder.classes_\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c319c62-e799-4d42-92c4-5c5065a5ce86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
